{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Notebook 1: Data Extraction, Transformation and Loading**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Import raw data from Kaggle into a dataframe\n",
        "* Clean data to remove duplicate values and remove outliers\n",
        "* Identify and handle missing data\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Raw data files from [CO2 Emissions Dataset](https://www.kaggle.com/datasets/shreyanshdangi/co-emissions-across-countries-regions-and-sectors/data)\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generates clean_data.csv for use in hypothesis testing and visualisations\n",
        "\n",
        "## Explanation of Terms\n",
        "\n",
        "The interquartile range (IQR) was used to identify countries with high levels of cummulative or current CO2 emissions for inclusion in the analysis.<br>\n",
        "IQR is a statistical measure of dispersion, with data being split into 4 quartiles. Q3 was used for our selection, meaning that we keep the top 75% of countries for each measure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import packages needed to run the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ydata_profiling import ProfileReport\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import raw data into dataframe, ready for processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "# set path to data file\n",
        "path = \"../raw_data/data.csv\"\n",
        "\n",
        "# assign data to dataframe\n",
        "df_raw = pd.read_csv(path)\n",
        "\n",
        "# display dataframe\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Initital Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First I will do some initital data cleaning steps:\n",
        "1. Check for duplicate rows and remove if found\n",
        "2. Limit data to country information to fit the requirements of the analysis\n",
        "3. Limit data to last 50 years to make analysis easier to manage and findings more relevant to current times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop any duplicate rows\n",
        "df_raw.drop_duplicates(inplace=True)\n",
        "\n",
        "# select rows where Description is equal to Country\n",
        "df_raw = df_raw.loc[df_raw['Description'] == \"Country\"]\n",
        "\n",
        "# limit data to last 50 years\n",
        "df_raw = df_raw.loc[df_raw[\"year\"] >= 1975]\n",
        "\n",
        "df_raw.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next step is to drop columns which aren't needed for analysis, this will reduce the size of the data file making processing more efficient and the data easier to work with.<br>\n",
        "(GPT-5 was used to format column names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a list of columns names to keep in the dataset\n",
        "req_columns = [\"Name\", \"iso_code\", \"year\", \"population\", \"gdp\", \"primary_energy_consumption\", \"co2\", \"co2_including_luc\", \"total_ghg\", \"co2_growth_abs\", \"co2_growth_prct\",\n",
        "               \"co2_per_capita\", \"co2_per_gdp\", \"energy_per_capita\", \"energy_per_gdp\", \"cement_co2\", \"coal_co2\", \"flaring_co2\", \"gas_co2\", \"land_use_change_co2\", \"oil_co2\",\n",
        "               \"share_global_co2\", \"share_global_co2_including_luc\", \"cumulative_co2\", \"cumulative_co2_including_luc\", \"share_global_cumulative_co2\"]\n",
        "\n",
        "# create a new dataframe with only the required columns\n",
        "df_trimmed = df_raw[req_columns].copy()\n",
        "df_trimmed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last step is to select countries with the highest cummulative or current co2 emission levels, so that analysis can focus on the most impactful countries for emissions.<br>\n",
        "(Method and steps for data filtering suggested by copilot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a temporary dataframe with only the columns of interest for the most recent year\n",
        "df_temp = df_trimmed.loc[df_trimmed['year'] == 2023, ['Name','cumulative_co2', 'co2']]\n",
        "\n",
        "# calculate interquartile range, we are interested in the top 75% of values\n",
        "iqr_cum, iqr_cap = df_temp[['cumulative_co2', 'co2']].quantile(.75)\n",
        "\n",
        "# select countries where values are higher than IQR3\n",
        "df_filtered = df_temp[(df_temp['cumulative_co2'] >= iqr_cum) | (df_temp['co2'] >= iqr_cap)]\n",
        "\n",
        "# remove countries which do not meet the thresholds from the main dataset\n",
        "df_trimmed = df_trimmed[df_trimmed['Name'].isin(df_filtered['Name'])].copy()\n",
        "df_trimmed['Name'].value_counts().shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Initial Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next I will use y-data profiling to learn more about the dataset and find out if any further transformation is required to prepare it for use in visualisations.<br>\n",
        "The preview from the table above shows some columns that will need to be rescaled (e.g. population and gdp) and some missing data to be handled (e.g. primary_energy_consumption and trade_co2_share)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "profile = ProfileReport(df=df_trimmed, minimal=True)\n",
        "profile.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This has highlighted missing data in several columns which will need to be handled.<br>\n",
        "Population data would benefit from being rescaled to a more human readable format. <br>\n",
        "GDP is largly reported in scientific notation format and is not intuitive to interpret for most people, it could be beneficial to rescale this column. It is currently reported in $, but grouping as thousands or millions may make more sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basic Descriptives and Visualisations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section includes basic data descriptives and visualisations which are used to inform final data transformations.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Population\n",
        "Population data would benefit from being scaled to a more human readable format<br>\n",
        "(GPT-5 was used to refactor the dataframe creation code and https://stackoverflow.com/questions/40347689/dataframe-describe-suppress-scientific-notation was used to reformat the descriptives output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create dataframe with population data for 2023\n",
        "df_temp = df_trimmed.loc[df_trimmed['year'] == 2023, ['Name','population']]\n",
        "# reset dataframe index\n",
        "df_temp.reset_index(drop=True)\n",
        "\n",
        "# create descriptives for population and global co2 share, making sure format doesn't include scientific notation\n",
        "df_temp.describe().apply(lambda s: s.apply('{0:.2f}'.format))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These values suggest that rescaling population into millions would be a good way to rescale the data<br>\n",
        "(Code adapted from https://stackoverflow.com/questions/43675014/dividing-a-dataframe-column-and-then-rounding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a new population column by dividing population values by 1m and round to 2 decimal places\n",
        "df_trimmed['pop(m)'] = df_trimmed['population'].div(1000000).round(2)\n",
        "df_trimmed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is a histogram to visualise population counts after rescaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# histogram of population\n",
        "plt.hist(df_trimmed['pop(m)'])\n",
        "plt.title(\"Histogram of Population Counts\")\n",
        "plt.xlabel(\"Population in millions\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This chart shows that most population data is on the lower end of the scale, even after removing low impact countries from analysis.<br>\n",
        "There are a few outlier countries with very high populations, but as they are likely to also have higher emission levels these will remain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GDP\n",
        "GDP data would benefit from being rescaled to a more human friendly format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_trimmed['gdp'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before rescaling I will need to handle missing values for GDP.<br>\n",
        "(Code provided by GPT-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show countries with missing gdp in df_trimmed and how many missing rows per country\n",
        "missing_counts = df_trimmed[df_trimmed['gdp'].isna()].groupby('Name').size().sort_values(ascending=False)\n",
        "print(f\"Total countries with missing gdp: {missing_counts.shape[0]}\")\n",
        "missing_counts.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All countries have some missing gdp data, but each country has at least some gdp data.<br>\n",
        "I will use the fill up method to fill in missing values as it is assumed that earlier years are more likely to contain missing values than more recent ones. <br>\n",
        "This method may skew earlier data as gdp is likely to increase over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# upward fill missing GDP values\n",
        "df_trimmed['gdp'].bfill(inplace=True)\n",
        "# perform a forward fill to catch any remaining missing values\n",
        "df_trimmed['gdp'].ffill(inplace=True)\n",
        "df_trimmed['gdp'].info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that all missing data has been filled we can rescale GDP.<br>\n",
        "The minimum value is 142,968,000 (142 million) and the highest number is 26,966,000,000,000 (26 trillion).<br>\n",
        "Rescaling GDP into billion dollars seems like a good method for producing values that can be understood intuitively.<br>\n",
        "(https://www.calculatorsoup.com/calculators/math/scientific-notation-converter.php was used to convert scientific notation into real numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create new gdp column by dividing gdp by 1b and rounding to 2 decimal places\n",
        "df_trimmed['gdp($b)'] = df_trimmed['gdp'].div(1000000000).round(2)\n",
        "df_trimmed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's a histogram to show GDP after rescaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# histogram of population\n",
        "plt.hist(df_trimmed['gdp($b)'])\n",
        "plt.title(\"Histogram of GDP Counts\")\n",
        "plt.xlabel(\"GDP in $billions\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly to population most values are on the lower end of the scale, with a few higher outliers which we would expect to also contribute more highly to emissions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Missing Values\n",
        "Now I need to identify and handle any remaining missing data.<br>\n",
        "(Code improved by GPT-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show columns in df_trimmed that have missing values\n",
        "missing = df_trimmed.isnull().sum()\n",
        "missing = missing[missing > 0].sort_values(ascending=False)\n",
        "missing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are several columns with missing data, I want to identify any countries with no data for these metrics as these would not be good targets for back or forward fill methods.<br>\n",
        "(Code written by GPT-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show countries with high missing values for a metric\n",
        "missing_over_40 = []\n",
        "\n",
        "for col in missing.index:\n",
        "    counts = df_trimmed[df_trimmed[col].isna()].groupby('Name').size()\n",
        "    for name, cnt in counts.items():\n",
        "        if cnt > 40:\n",
        "            missing_over_40.append((name, col, int(cnt)))\n",
        "\n",
        "if missing_over_40:\n",
        "    for name, col, cnt in missing_over_40:\n",
        "        print(f\"{name}: column '{col}' missing {cnt} rows\")\n",
        "else:\n",
        "    print(\"No country has more than 40 missing values for any column\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "My data owner (a copilot persona) has said that this missing data should not be imputed for accountability metrics, instead this missing data will be flagged in relevant visualisations.<br>\n",
        "The rest of the missing values will be filled using the back/forward fill method using existing values, this may lead to some skew in time series analysis.<br>\n",
        "(GPT-5 suggested code to guard against division errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fill missing energy values\n",
        "df_trimmed['primary_energy_consumption'].bfill(inplace=True)\n",
        "# perform a forward fill to catch any remaining missing values\n",
        "df_trimmed['primary_energy_consumption'].ffill(inplace=True)\n",
        "\n",
        "# gaps in calculated columns can now be filled using the imputed data\n",
        "calc_col = ['energy_per_gdp', 'co2_per_gdp', 'energy_per_capita']\n",
        "\n",
        "# calculate and fill missing values\n",
        "df_trimmed['energy_per_gdp'] = df_trimmed['energy_per_gdp'].fillna(df_trimmed['primary_energy_consumption'] / df_trimmed['gdp'])\n",
        "df_trimmed['co2_per_gdp'] = df_trimmed['co2_per_gdp'].fillna(df_trimmed['co2'] / df_trimmed['gdp'])\n",
        "df_trimmed['energy_per_capita'] = df_trimmed['energy_per_capita'].fillna(df_trimmed['primary_energy_consumption'] / df_trimmed['population'])\n",
        "\n",
        "# guard against division issues\n",
        "df_trimmed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# show columns in df_trimmed that have missing values\n",
        "missing = df_trimmed.isnull().sum()\n",
        "missing = missing[missing > 0].sort_values(ascending=False)\n",
        "missing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now I need to handle the final missing data, these values will again be back/forward filled but North Korea and Taiwan will be handled separately.<br>\n",
        "(Code generated by GPT-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define countries to exclude from imputation\n",
        "exclude = ['Taiwan', 'North Korea']\n",
        "\n",
        "# determine columns with missing values\n",
        "cols_with_na = df_trimmed.columns[df_trimmed.isna().any()].tolist()\n",
        "\n",
        "# perform bfill then ffill only for rows not in `exclude`\n",
        "mask = ~df_trimmed['Name'].isin(exclude)\n",
        "df_trimmed.loc[mask, cols_with_na] = df_trimmed.loc[mask, cols_with_na].bfill().ffill()\n",
        "\n",
        "# replace any inf values that may have arisen, and show remaining missing counts\n",
        "df_trimmed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df_trimmed[cols_with_na].isnull().sum().sort_values(ascending=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally I will fill in flaring CO2 values only for Taiwan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# backfill flaring data, Taiwan is the only country with missing values, so we don't need any filtering\n",
        "df_trimmed['flaring_co2'].bfill(inplace=True)\n",
        "# forward fill in case any values were missed\n",
        "df_trimmed['flaring_co2'].ffill(inplace=True)\n",
        "df_trimmed.isnull().sum().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The only remaining missing values are those flagged earlier as not to be imputed.<br>\n",
        "As the last step in data transformation I will drop the raw values for population and GDP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop unnecessary columns\n",
        "df_trimmed.drop(['gdp'], axis=1, inplace=True)\n",
        "df_trimmed.drop(['population'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Output Cleaned Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save cleaned data to a csv file ready to be used for visualisations, hypothesis testing, model building and dashboarding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "# export cleaned data to csv\n",
        "path = \"../raw_data/clean_data.csv\"\n",
        "df_trimmed.to_csv(path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the end of Notebook 1, users should open 02_statistics_and_visualisations.ipynb to continue analysis."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
